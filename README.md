# Multilayer perceptron from scratch
# Project overview:
The project is an implementation of a multilayer perceptron.
The layers, network and activation functions are all implemented from scratch using only numpy.

## Optimisation techniques
The project implements 2 optimization techniques:

1. Standart backpropagation using the stochastic gradient descent algorithm.
2. An experimental Genetic aproach.

Both methods are currently functional, but both still have a lot of room for improvement.

## Activation functions
The project implements 5 activation functions and their derivatives:

1. Sigmoid
2. ReLU
3. Leaky ReLU
4. Tanh
5. Tanh (approximated)


## Sources

- [Standford, Additional Notes on Backpropagation ](https://cs229.stanford.edu/notes-spring2019/backprop.pdf)
- [Backpropagation ](https://en.wikipedia.org/wiki/Backpropagation)
